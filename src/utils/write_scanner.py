def write_scanner(filename, regex, yalex_code):
    """
    Writes a scanner file based on the provided regex and Yalex code.

    Args:
        filename (str): The name of the file to be written.
        regex (str): The regular expression used for tokenization.
        yalex_code (list): A list of tuples containing the Yalex code and corresponding tokens.
    """

    lines = [
        "# This file was generated by the Yalex compiler.",
        "# Do not modify this file.",
        "",
        "import os",
        "from algorithms.direct_construction import dfa_direct_construction",
        "from yal_processing.simulate_yalex_file import simulate_yalex_file",
        "from utils.path_cleaner import replace_slash_with_backslash",
        "",
        f"dfa = dfa_direct_construction({regex}, {yalex_code})",
        "",
        "raw_file = input(\"Enter the path of the yal testing file: \")",
        "test_file = replace_slash_with_backslash(raw_file)",
        "file_name = os.path.basename(test_file).split('.')[0]",
        "output_file = f\"./out/{file_name}_tokens.txt\"",
        "output_type = \"analysis\"",
        "",
        "def return_token(found_token):",
    ]

    # Ensure token definitions and function blocks are correctly written
    token_definitions = set()
    for code, token in yalex_code:
        if token:  # Ensure the token is not empty
            token_definitions.add(token)
            lines.append(f"    {token} = \"{token}\"")
    lines.append("")

    for code, token in yalex_code:
        if token:  # Ensure the token is not empty
            lines.append(f"    if (found_token == {token}):")
            lines.append(f"        {code}")
    lines.append("")
    lines.append("simulate_yalex_file(test_file, output_file, dfa, output_type, return_token)")

    # Write the scanner file.
    with open(filename, "w+") as file:
        for line in lines:
            file.write(line + "\n")

    print(f"\nScanner written successfully.\n")